## [7000长文：一文读懂Agent，大模型的下一站](https://zhuanlan.zhihu.com/p/678046050?utm_medium=social&utm_psn=1800627465938411520&utm_source=qq)

## [深入剖析了AI Agent这一前沿科技领域的全貌](https://www.zhihu.com/question/653727660/answer/3574372576)

## [生成式 AI 的发展方向，应当是 Chat 还是 Agent？](https://www.zhihu.com/question/637090848/answer/3363233578)

## [大模型Agent的核心还是prompt？](https://www.zhihu.com/question/628670548/answer/3284156768?utm_medium=social&utm_psn=1800595210364932096&utm_source=qq)

## [AI Agent基础知识，从这篇博文学起](https://www.53ai.com/news/LargeLanguageModel/2024071891583.html)
## [一文读懂：AI Agent究竟是什么？](https://m.huxiu.com/article/1935893.html)

## [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
![Alt text](assets/agent/image-4.png)
![Alt text](assets/agent/image.png)
这张图片展示了自主智能体的分类和特性，分为“更像人的 AI”和“更像工具的 AI”两类，以及“System 1（快思考）”和“System 2（慢思考）”两种系统。


### 规划 (Planning)

在处理复杂任务时，自主智能体（Agent）需要了解所需的步骤并提前进行规划。规划的关键在于任务分解，通过将复杂的任务拆解成更小、更易管理的子任务，使得智能体能够逐步解决问题。

#### 1. 思维链 (Chain-of-Thoughts, CoT)

**定义**：思维链是一种技术，通过逐步思考的方式来增强模型在复杂任务上的表现。

**功能**：
- **逐步分解任务**：通过 "think step by step" 的方式，模型可以将复杂的问题拆解为一系列简单的子任务。
- **增强理解和推理**：逐步分解任务可以帮助模型更好地理解问题的各个方面，提高推理能力。

**示例**：
- **简单提示**：例如“XYZ的步骤”，模型会根据提示逐步分解任务。
- **特定任务指令**：例如“写小说大纲”，模型会根据指令分解出各个章节和内容要点。

#### 2. 思维树 (Tree of Thoughts)

**定义**：思维树是对思维链的扩展，通过在每个步骤中探索多种推理可能性，生成树状结构。

**功能**：
- **多种推理可能性**：在每个步骤中，模型会探索多种可能的推理路径，形成树状结构。
- **搜索算法**：可以使用广搜（BFS）或深搜（DFS）进行搜索，每个状态由分类器或多数投票评估，以找到最优解。

**示例**：
- **广搜（BFS）**：从根节点开始，逐层向下探索每个可能的推理路径。
- **深搜（DFS）**：从根节点开始，深入探索一条路径到底，然后回溯并探索其他路径。

#### 3. 任务分解方法

**方法**：
- **简单提示**：如“XYZ的步骤”，通过明确的提示引导模型分解任务。
- **特定任务指令**：如“写小说大纲”，提供具体指令帮助模型分解任务。
- **人类输入**：通过人类输入来辅助任务分解，提供更精确的指导和反馈。

**示例**：
- **简单提示**：模型接收到提示“准备晚餐的步骤”，会分解为“购买食材”、“准备食材”、“烹饪”、“摆盘”等步骤。
- **特定任务指令**：模型接收到指令“写小说大纲”，会分解为“设定场景”、“定义角色”、“设计情节”等。

#### 4. LLM+P 方法（略）

**定义**：LLM+P 方法将规划步骤外包给外部工具，这种方法可以有效利用外部资源和工具来完成任务分解和规划。

**功能**：
- **外部工具协助**：利用外部工具的专长来完成复杂的规划和任务分解。
- **提高效率**：通过外包部分任务，模型可以集中精力在其擅长的领域，提高整体效率。

**示例**：
- **使用外部工具**：在软件开发任务中，模型可以调用专门的项目管理工具来分解和规划开发步骤。

### 总结

通过思维链和思维树技术，自主智能体能够有效地分解和规划复杂任务。简单提示和特定任务指令可以帮助模型逐步分解任务，而LLM+P方法则通过外包规划步骤进一步提高了模型的效率和能力。这些技术和方法的结合，使得自主智能体在处理复杂任务时表现更加出色。

---


### 反思 (Self-Reflection)

反思是指自主智能体（Agent）通过回顾和改进过去的行动和决策来提高自身性能的过程。这一过程帮助智能体在处理复杂任务时不断学习和进步。

#### 1. ReAct 方法

**定义**：ReAct 方法通过将动作空间扩展为特定于任务的离散动作和语言空间的组合，将推理和动作集成在大型语言模型 (LLM) 中。

**功能**：
- **动作与推理结合**：智能体不仅能执行具体动作，还能生成相应的推理痕迹。
- **环境交互**：例如，智能体可以使用维基百科搜索 API 来获取信息，同时生成相应的推理过程。

**示例**：
- **任务**：查找某个历史事件的详细信息。
  - **步骤**：
    1. **生成查询**：智能体生成关于该历史事件的查询。
    2. **执行查询**：通过维基百科搜索 API 执行查询。
    3. **生成推理痕迹**：记录查询结果和推理过程，如“根据维基百科，事件发生在X年Y月，主要人物包括A和B”。

#### 2. Reflexion 框架

**定义**：Reflexion 框架为智能体配备了动态记忆和自我反思能力，通过展示失败的轨迹和理想反思的示例，帮助智能体在工作记忆中存储反思，以指导未来的计划。

**功能**：
- **动态记忆**：智能体能够记住并回顾过去的行动和决策。
- **自我反思**：智能体通过分析失败案例和理想反思来改进未来的决策。

**示例**：
- **任务**：解决复杂的数学问题。
  - **步骤**：
    1. **初始解答**：智能体尝试解决问题，但解答错误。
    2. **失败分析**：分析解答过程，发现计算步骤有误。
    3. **理想反思**：通过对比正确解答步骤，记录下正确的推理过程。
    4. **存储反思**：将反思内容存储在动态记忆中。
    5. **改进解答**：在下次遇到类似问题时，参考存储的反思内容，避免相同错误。

#### 3. 启发式功能

**定义**：启发式功能决定何时停止当前的规划轨迹，因为它变得效率低下或包含幻觉。

**功能**：
- **效率评估**：智能体评估当前规划轨迹的效率，决定是否继续。
- **避免无效行动**：识别并避免无效或错误的行动序列。

**示例**：
- **任务**：优化物流配送路径。
  - **步骤**：
    1. **初始规划**：智能体规划配送路径。
    2. **效率评估**：评估当前路径是否高效。
    3. **识别低效**：发现当前路径在某些节点上出现重复或无效行动。
    4. **调整路径**：基于启发式功能，重新规划路径，避免无效节点。
    5. **改进决策**：在未来的配送任务中，参考此次反思，避免类似低效路径。

### 总结

反思能力使自主智能体能够不断改进自身的决策和行动质量。通过 ReAct 方法，智能体能够结合推理和具体行动，与环境进行有效交互。Reflexion 框架则通过动态记忆和自我反思，帮助智能体在失败中学习和进步。启发式功能则确保智能体在规划中避免无效或错误的行动序列。这些机制共同提高了智能体在复杂环境中的决策和学习能力。

---

![Alt text](assets/agent/image-5.png)

这张图片详细展示了记忆模块的结构，将记忆分为感官记忆、短期记忆和长期记忆，并进一步细分了每种记忆的类型。下面是对每个部分的详细解释：

### 记忆模块

#### 1. 感官记忆 (Sensory Memory)
感官记忆是记忆的最初阶段，负责暂时存储来自感官的信息。感官记忆通常只能持续几秒钟，主要包括以下几种类型：

- **Iconic Memory（视觉记忆）**：
  - **定义**：短暂存储视觉信息的记忆。
  - **功能**：保持视觉信息的瞬时印象，帮助我们识别和处理视觉刺激。
  - **例子**：快速闪过的图像仍能在我们脑海中保持一瞬间。

- **Echoic Memory（听觉记忆）**：
  - **定义**：短暂存储听觉信息的记忆。
  - **功能**：保持听觉信息的瞬时印象，帮助我们识别和处理声音。
  - **例子**：别人对你说话后的几秒钟内，你仍能回忆起他说的最后几句话。

- **Haptic Memory（触觉记忆）**：
  - **定义**：短暂存储触觉信息的记忆。
  - **功能**：保持触觉信息的瞬时印象，帮助我们识别和处理触觉刺激。
  - **例子**：你刚摸到一个物体的质感在几秒钟内还能记得。

#### 2. 短期记忆 (Short-term Memory, Working Memory)
短期记忆，也称为工作记忆，是暂时存储和处理信息的地方，持续时间较短，一般在几十秒内。

- **定义**：短期存储我们正在使用或思考的信息。
- **功能**：用于执行复杂认知任务，如学习、推理和理解。
- **例子**：记住一个刚刚听到的电话号码，直到你拨打完为止。

#### 3. 长期记忆 (Long-term Memory)
长期记忆是持久存储信息的地方，可以存储几天到几十年，甚至终生。长期记忆进一步分为显性记忆和隐性记忆：

- **显性/陈述性记忆 (Explicit/Declarative Memory)**：
  - **定义**：可意识到并能明确表达的记忆。
  - **子类型**：
    - **情景记忆 (Episodic Memory)**：关于个人经历和事件的记忆。
      - **例子**：你上次生日的庆祝活动。
    - **语义记忆 (Semantic Memory)**：关于事实和概念的记忆。
      - **例子**：巴黎是法国的首都。

- **隐性/程序性记忆 (Implicit/Procedural Memory)**：
  - **定义**：不需要意识到的、无意识的技能和习惯的记忆。
  - **功能**：用于自动化的动作和技能。
  - **例子**：骑自行车或打字的技能。

### 总结
记忆模块详细展示了人类记忆的复杂性和多样性，从最初的感官记忆，到能够暂时存储信息的短期记忆，再到能够持久保存信息的长期记忆。每种记忆类型在我们日常生活中都起着重要作用，帮助我们处理和存储各种信息，提高我们的认知和行动能力。


---


### 短期记忆与长期记忆在Agent中的应用

#### 1. 短期记忆 (Short-term Memory)

**定义**：短期记忆主要涉及模型的上下文学习，利用模型的即时计算能力来处理和存储任务执行过程中的临时信息。

**功能**：
- **即时信息处理**：短期记忆能够快速存储和处理当前任务所需的信息。
- **上下文理解**：帮助Agent理解当前任务的上下文，从而做出即时决策。

**应用**：
- **对话系统**：在对话过程中，短期记忆存储当前对话的上下文，使Agent能够生成连贯的回答。
- **任务执行**：在执行复杂任务时，短期记忆帮助Agent保持当前任务的状态和步骤。

**示例**：
- **对话场景**：用户连续提出几个相关问题，Agent利用短期记忆记住之前的问题和回答，从而提供连贯的对话。
- **即时决策**：在游戏中，Agent通过短期记忆记住当前环境状态和最近的动作，从而做出下一步决策。

#### 2. 长期记忆 (Long-term Memory)

**定义**：长期记忆提供了一种机制，使Agent能够长时间保留和回忆信息。通常通过外部向量存储和快速检索实现。

**功能**：
- **信息持久化**：长期记忆允许Agent存储和访问过去的经验和知识。
- **知识积累**：长期记忆使Agent能够利用过去的经验来辅助当前的决策和任务执行。

**应用**：
- **知识库**：Agent可以访问大型知识库，检索相关信息来回答用户问题或执行任务。
- **复杂任务**：在复杂任务执行中，长期记忆帮助Agent利用过去的经验和策略来优化当前任务。

**示例**：
- **知识问答**：用户询问一个历史事件，Agent从长期记忆中检索相关信息，提供准确的回答。
- **任务优化**：在软件开发任务中，Agent利用长期记忆中的过去项目经验，优化当前的开发流程。

#### 记忆系统在Agent决策中的角色

**角色**：
- **记住过去的行动和结果**：帮助Agent记住并回顾过去的操作，了解哪些策略有效，哪些无效。
- **信息检索**：在需要时快速检索相关信息，提高任务执行的效率和准确性。

**示例**：
- **处理复杂问题**：Agent需要解决一个复杂的编程问题，可以检索长期记忆中的相关解决方案和经验，辅助当前决策。
- **优化策略**：通过回顾过去的失败和成功，Agent可以调整当前的策略，提高任务成功率。

#### 挑战

**挑战**：
- **信息管理和检索**：如何有效地管理和检索大量信息是一个关键问题。
- **信息准确性和相关性**：确保存储的信息是准确且相关的，对Agent的决策至关重要。
- **计算能力和资源限制**：设计和实现记忆系统时需要考虑Agent的计算能力和资源限制。

**解决方法**：
- **外部数据库或知识库**：使用外部向量数据库（vector database）来存储和管理信息，Agent通过查询这些数据库来检索所需的信息。

**示例**：
- **外部数据库**：Agent连接到一个外部知识库，如Wolfram Alpha或Google Knowledge Graph，通过查询这些数据库来获取所需的信息。
- **向量数据库**：使用向量数据库，如FAISS或Annoy，来存储和快速检索长时间积累的经验和知识。

### 总结

记忆系统在Agent的决策过程中扮演着重要角色。短期记忆帮助Agent在处理当前任务时快速获取和处理必要信息，而长期记忆提供了一个持久的知识库，使Agent能够利用过去的经验和知识来辅助决策和任务执行。尽管记忆系统在实际应用中面临挑战，但通过有效的管理和检索机制，Agent能够显著提高任务执行的效率和准确性。

![Alt text](assets/agent/image-6.png)

通过利用外部工具和API，Agent可以显著扩展其功能，从而提高处理复杂任务的能力。下面我们详细解释HuggingGPT的工作流程，并举例说明如何通过该框架完成任务。

### HuggingGPT 框架的四个阶段

1. **任务规划 (Task Planning)**
   - **解释**：ChatGPT作为任务规划器，首先分析用户的请求，并将其分解成可解决的子任务。
   - **示例**：用户请求描述一张图片并计算其中有多少个物体。ChatGPT分析请求，将其分解为图像描述和物体计数两个子任务。

2. **模型选择 (Model Selection)**
   - **解释**：ChatGPT根据任务描述选择适当的模型。这些模型托管在HuggingFace平台上。
   - **示例**：ChatGPT选择了两个模型，一个用于图像分类（facebook/detr-resnet-101），一个用于图像描述（nlpconnect/vit-gpt2-image-captioning）。

3. **任务执行 (Task Execution)**
   - **解释**：调用并执行选择的模型，将结果反馈给ChatGPT。
   - **示例**：图像分类模型识别出图片中的长颈鹿和斑马，并计算出物体数量；图像描述模型生成对图片的文字描述。

4. **响应生成 (Response Generation)**
   - **解释**：ChatGPT整合所有模型的预测结果，生成最终响应。
   - **示例**：综合图像分类和描述的结果，ChatGPT生成完整的回答，描述图片内容并报告物体数量。

### 具体示例：描述图片并计算物体数量

#### 用户请求
“请描述这张图片，并计算其中有多少个物体。”

#### 任务规划
ChatGPT分析请求，将其分解为两个子任务：
1. 描述图片内容
2. 计算图片中的物体数量

#### 模型选择
1. **图像分类模型**：facebook/detr-resnet-101
   - **功能**：检测图片中的物体，并分类。
2. **图像描述模型**：nlpconnect/vit-gpt2-image-captioning
   - **功能**：生成图片的文字描述。

#### 任务执行
1. **执行图像分类模型**：
   - 结果：识别出图片中的长颈鹿和斑马，并计算出物体数量。
   - 具体输出：长颈鹿（5个），斑马（4个）。

2. **执行图像描述模型**：
   - 结果：生成对图片的描述。
   - 具体输出：图片描述为“在草地上吃草的一群长颈鹿和斑马。”

#### 响应生成
ChatGPT将两个模型的结果整合，生成最终响应：
- “这张图片描述了一群在草地上吃草的长颈鹿和斑马。图片中共检测到9个物体，其中包括5个长颈鹿和4个斑马。”

### 总结
HuggingGPT通过有效调用和利用外部工具和API，实现了复杂任务的分解、模型选择、任务执行和响应生成。这种框架显著提高了Agent处理复杂任务的能力，使其能够高效准确地完成用户请求。

通过这种方法，Agent不仅能扩展其功能，还能在处理复杂任务时提高效率和准确性。实际应用中，类似API-Bank的工具也可以发挥类似作用，提供丰富的模型和API资源，进一步提升Agent的能力。
### 分类解释：

1. **更像工具的 AI（中立，友好）**
   - **System 1（快思考）**：无状态、简单、一问一答
     - GPT-3
     - Stable Diffusion
     - ChatGPT
     - Office Copilot

   - **System 2（慢思考）**：有状态、复杂、交互式、规划
     - AutoGPT
     - MetaGPT

2. **更像人的 AI（个性，心情，自我认知）**
   - **System 1（快思考）**：无状态、简单、一问一答
     - Character.ai
     - Pi
     - Talkie/星野

   - **System 2（慢思考）**：有状态、复杂、交互式、规划
     - Ash（黑镜）
     - 斯坦福 AI 小镇
     - 图丫丫（流浪地球2）
     - Samantha（Her）
     - 贾维斯（钢铁侠）

### 主要内容：

- **System 1** 代表快思考的 AI，主要特点是无状态、简单、一问一答的交互方式。这类 AI 更像工具，提供中立、友好的帮助。
- **System 2** 代表慢思考的 AI，具备有状态、复杂、交互式的特点，可以进行规划和复杂决策。这类 AI 更像人，具有个性、心情和自我认知能力。
  
通过这些分类，可以看出不同类型的自主智能体在功能和应用上的差异。System 1 适用于简单直接的任务，而 System 2 则适用于复杂的情境和需要长期规划的任务。

---

图片中的术语部分列出了几种与大模型自主智能体相关的术语，这些术语分别描述了不同类型或功能的自主智能体。下面是对这些术语的详细解释：

1. **Language agent（语言智能体）**
   - 这是指使用自然语言处理技术与用户进行交互的智能体。它能够理解和生成自然语言，用于各种对话系统和语言驱动的任务。

2. **LLM-empowered agents（大语言模型驱动的智能体）**
   - 这些智能体依赖于大型语言模型（LLM）的能力来执行任务。大型语言模型如GPT-3具有强大的语言理解和生成能力，使这些智能体能够处理复杂的语言任务。

3. **LLM powered autonomous agents（大语言模型支持的自主智能体）**
   - 这种智能体不仅使用大型语言模型进行语言处理，还能自主地进行任务规划和执行。它们可以在没有明确指令的情况下，通过理解上下文和推理来完成任务。

4. **Language enabled agents（语言增强智能体）**
   - 这些智能体利用语言处理技术来增强其功能，可以通过语言进行命令和控制，适用于各种需要语言交互的场景。

5. **LLM based agents（基于大语言模型的智能体）**
   - 这类智能体的核心技术是大语言模型，它们使用这种技术来执行各种任务，包括对话、信息检索和内容生成等。

### 总结：
这些术语描述了利用语言处理和大语言模型技术来驱动和增强智能体功能的不同方式。它们强调了语言在自主智能体中的核心作用，从简单的语言交互到复杂的自主决策和任务执行。通过理解这些术语，我们可以更好地理解大模型自主智能体在不同应用场景中的作用和实现方式。

---
![Alt text](assets/agent/image-1.png)



在你的示例代码中，`agent.run` 函数被用来执行不同的任务。每个任务都包含一个指令和相关的数据（如图像、文本或文档）。这种设计可能用于一个能够理解自然语言指令并执行相应操作的智能代理。以下是每个任务的详细解释：



![Alt text](assets/agent/image-2.png)
![Alt text](assets/agent/image-3.png)
### 示例代码

```python
agent.run("Caption the following image", image=image)
agent.run("Read the following text out loud", text=text)
agent.run(
    "In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?",
    document=document,
)
```

### 详细解释

#### 1. `agent.run("Caption the following image", image=image)`

**任务**：为以下图像生成描述（Caption）。

- **指令**： `"Caption the following image"` 告诉代理它需要为提供的图像生成描述。
- **数据**： `image=image` 表示传递一个图像对象给代理。

**解释**：
- 该任务的目的是通过自然语言处理和计算机视觉技术分析图像内容，并生成一段描述该图像的文字。例如，如果图像中有一只猫在阳光下晒太阳，代理可能会生成描述“阳光下的一只猫”。

#### 2. `agent.run("Read the following text out loud", text=text)`

**任务**：大声朗读以下文本。

- **指令**： `"Read the following text out loud"` 告诉代理它需要朗读提供的文本。
- **数据**： `text=text` 表示传递一段文本数据给代理。

**解释**：
- 该任务的目的是使用文本到语音转换（Text-to-Speech, TTS）技术，将提供的文本转换为语音并朗读出来。例如，如果文本是“Hello, how are you?”，代理会生成相应的语音输出，朗读这段文字。

#### 3. `agent.run("In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?", document=document)`

**任务**：在以下文档中，确定 TRRF 科学咨询委员会会议的地点。

- **指令**： `"In the following 'document', where will the TRRF Scientific Advisory Council Meeting take place?"` 告诉代理它需要在提供的文档中查找 TRRF 科学咨询委员会会议的地点。
- **数据**： `document=document` 表示传递一个文档对象给代理。

**解释**：
- 该任务的目的是通过自然语言处理技术分析提供的文档内容，并提取出有关 TRRF 科学咨询委员会会议地点的信息。例如，如果文档中包含“TRRF 科学咨询委员会会议将在华盛顿特区举行”，代理应该提取并返回“华盛顿特区”。

### 关键概念

1. **自然语言处理（NLP）**：
   - NLP 用于理解和生成人类语言。第一和第三个任务使用 NLP 技术来分析和生成文本。

2. **计算机视觉（CV）**：
   - CV 用于图像分析。第一个任务需要 CV 技术来理解图像内容并生成描述。

3. **文本到语音转换（TTS）**：
   - TTS 用于将文本转换为语音。第二个任务使用 TTS 技术来生成语音输出。

### 代理的工作流程

1. **解析指令**：
   - `agent.run` 方法首先解析传递的指令，确定需要执行的任务类型。

2. **处理数据**：
   - 根据指令类型，代理处理传递的数据（图像、文本或文档）。

3. **执行任务**：
   - 代理调用适当的技术（如 NLP、CV 或 TTS）来执行任务，并生成结果。

4. **返回结果**：
   - 最终，代理返回生成的结果，例如图像描述、语音输出或提取的信息。

这种设计使得智能代理能够处理多种类型的任务，通过解析自然语言指令和处理相应的数据，执行复杂的操作并返回结果。这在实际应用中可以用于自动化内容生成、信息提取和语音合成等多种场景。

---
这段文字解释了 Hugging Face Agents 和 Tools 的工作机制，特别是如何通过提示大型语言模型（LLM）生成代码，并在受限的环境中执行这些代码，以确保安全性。以下是每个部分的详细解释：

### Agents

**定义**：
- **Agents** 是大型语言模型（LLM），通过提示（prompting）来访问特定的工具集（Tools）。

**工作原理**：
- **生成代码示例**：大型语言模型擅长生成小代码示例。因此，API 利用这一特点，通过提示 LLM 生成使用工具集合的小代码示例。
- **根据任务和工具描述生成代码**：根据你提供的任务和工具的描述，LLM 会生成相关的代码。这些代码会使用特定的工具来完成任务。
- **访问工具文档**：这种方法允许 LLM 访问工具的文档，了解它们的期望输入和输出，以生成正确的代码。

### Tools

**定义**：
- **Tools** 是有名称和描述的单个函数。

**工作原理**：
- **描述和提示**：使用这些工具的描述来提示代理，让代理知道如何使用工具来执行查询语言中请求的操作。
- **原子化的工具**：这些工具非常简单和原子化，专注于一个非常简单的任务，不像 pipelines 那样将多个任务合并为一个。

### 代码执行

**如何执行代码**：
- **受限的 Python 解释器**：生成的小代码基于工具的输入在一个受限的 Python 解释器中执行。
- **仅执行指定工具**：只能执行你提供的工具和打印函数，从而限制了代码的执行范围。

**安全措施**：
1. **限制工具范围**：只能执行指定的工具和打印函数，防止任意代码执行。
2. **禁止属性查找和导入**：不允许任何属性查找或导入操作，防止常见的攻击。
3. **手动审核代码**：可以使用 `return_code=True` 参数，使代理只返回要执行的代码，由你决定是否执行。

**异常处理**：
- 如果生成的代码尝试执行非法操作，或者出现常规 Python 错误，执行将停止。

### 示例解释

假设你有一个生成和处理图像的任务，你可以使用以下代码：

#### 1. 定义工具

```python
def image_generator(prompt):
    # 模拟生成图像的工具
    return f"Generated image based on prompt: {prompt}"

def image_captioner(image):
    # 模拟生成图像描述的工具
    return f"Caption for image: {image}"

def text_to_speech(text):
    # 模拟文本转语音的工具
    return f"Audio output for text: {text}"
```

#### 2. 使用代理和工具完成任务

```python
from transformers import HfAgent

# 初始化代理
agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")

# 定义任务：生成河流和湖泊的图像并添加一个岛屿
picture = agent.run("Generate a picture of rivers and lakes.")
updated_picture = agent.run("Transform the image in `picture` to add an island to it.", picture=picture)
```

#### 3. 强制使用特定工具生成图像

```python
# 直接生成在大海中游泳的大水獭的图像
agent.run("Draw me the picture of a capybara swimming in the sea")
```

#### 4. 强制使用特定情景

```python
# 强制代理使用 text-to-image 工具生成图像
agent.run("Draw me a picture of the `prompt`", prompt="a capybara swimming in the sea")
```

### 总结

1. **Agents**：通过提示访问特定工具集的 LLM，生成和执行小代码示例。
2. **Tools**：简单的、原子化的函数，用于完成特定任务。
3. **代码执行**：在受限的 Python 解释器中执行生成的代码，只允许指定的工具和打印函数。
4. **安全措施**：限制工具范围、禁止属性查找和导入、手动审核代码，并处理异常情况。

这种方法确保了代码执行的安全性，同时利用 LLM 的强大能力来生成和执行复杂任务。